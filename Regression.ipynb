{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is Simple Linear Regression?\n",
        "Ans- Simple Linear Regression is a statistical method used to model the relationship between one independent variable (X) and one dependent variable (Y) by fitting a straight line.\n",
        "     \n",
        "     Y = ùëöX + c\n",
        "\n",
        "#2. What are the key assumptions of Simple Linear Regression?\n",
        "Ans-\n",
        "a. Linearity between X and Y\n",
        "\n",
        "b. Independence of observations\n",
        "\n",
        "c. Homoscedasticity (constant variance of errors)\n",
        "\n",
        "d. Normality of residuals\n",
        "\n",
        "e. No significant outliers\n",
        "\n",
        "#3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "Ans-\n",
        "\n",
        "a. m represents the slope.\n",
        "\n",
        "b. It shows the change in Y for a one-unit increase in X.\n",
        "\n",
        "#4. What does the intercept c represent in the equation Y=mX+c?\n",
        "Ans-\n",
        "\n",
        "a. c is the value of Y when X = 0.\n",
        "\n",
        "b. It represents the starting point of the regression line.\n",
        "\n",
        "#5. How do we calculate the slope in Simple Linear Regression?\n",
        "Ans-\n",
        "\n",
        " m = ‚àë(X-XÀâ)*(Y-YÀâ) / (‚àë(X-XÀâ)2)\n",
        "\n",
        " #6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        " Ans- a. To find the best-fit line.\n",
        "\n",
        "b. It minimizes the sum of squared residuals (errors).\n",
        "\n",
        "#7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "Ans- a. R¬≤ measures the proportion of variance in Y explained by X.\n",
        "\n",
        "b. Values range from 0 to 1.\n",
        "\n",
        "c. Higher R¬≤ ‚Üí better model fit.\n",
        "\n",
        "#8. What is Multiple Linear Regression?\n",
        "Ans-\n",
        "\n",
        "Multiple Linear Regression models the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "Y = b0 + b1X1 + b2X2 +‚ãØ+ bnXn\n",
        "\n",
        "#9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans. a. Simple: one independent variable.\n",
        "\n",
        "b. Multiple: two or more independent variables.\n",
        "\n",
        "# 10. What are the key assumptions of Multiple Linear Regression?\n",
        "Ans -\n",
        "\n",
        "a. Linearity\n",
        "\n",
        "b. Independence of errors\n",
        "\n",
        "c. Homoscedasticity\n",
        "\n",
        "d. Normality of residuals\n",
        "\n",
        "e. No multicollinearity\n",
        "\n",
        "f. No influential outliers\n",
        "\n",
        "#11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "Ans- Heteroscedasticity occurs in a Multiple Linear Regression model when the variance of the error terms (residuals) is not constant across all levels of the independent variables.\n",
        "In other words, the spread of residuals increases or decreases as the predicted values change.\n",
        "\n",
        "How it affects the results of a Multiple Linear Regression model:\n",
        "\n",
        "a. Unreliable standard errors: Estimated standard errors become biased.\n",
        "\n",
        "b. Invalid hypothesis tests: t-tests and F-tests may give incorrect results.\n",
        "\n",
        "c. Incorrect confidence intervals: Confidence intervals may be too wide or too narrow.\n",
        "\n",
        "d. Inefficient estimates: Coefficient estimates remain unbiased but are not efficient (not minimum variance).\n",
        "\n",
        "e. Poor model reliability: Predictions may be less reliable, especially for larger or smaller values of predictors.\n",
        "\n",
        "#12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "Ans- a. Remove highly correlated variables.\n",
        "\n",
        "b. Use Variance Inflation Factor (VIF).\n",
        "\n",
        "c. Apply Ridge or Lasso regression.\n",
        "\n",
        "d. Use Principal Component Analysis (PCA).\n",
        "\n",
        "#13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "Ans-\n",
        "Common techniques include:\n",
        "\n",
        "a. Dummy Variable Encoding:\n",
        "\n",
        "i). Converts categories into 0s and 1s.\n",
        "\n",
        "ii). One category is used as a reference.\n",
        "\n",
        "b. One-Hot Encoding:\n",
        "\n",
        "i). Creates a binary column for each category.\n",
        "\n",
        "ii). Commonly used in machine learning.\n",
        "\n",
        "c. Label Encoding:\n",
        "\n",
        "i). Assigns numerical labels (0, 1, 2, ‚Ä¶).\n",
        "\n",
        "ii). Suitable for ordinal variables only.\n",
        "\n",
        "d. Ordinal Encoding:\n",
        "\n",
        "i). Preserves the natural order of categories.\n",
        "\n",
        "ii). Example: Low = 1, Medium = 2, High = 3\n",
        "\n",
        "e. Binary Encoding:\n",
        "\n",
        "i). Converts categories into binary format.\n",
        "\n",
        "ii). Useful when there are many categories.\n",
        "\n",
        "#14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "Ans- a. Interaction terms show how the effect of one variable depends on another.\n",
        "\n",
        "Example: X1 * X2\n",
        "\n",
        "#15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "Ans. a. Simple Linear Regression:\n",
        "\n",
        "i). Intercept = value of Y when X = 0.\n",
        "\n",
        "ii). Often easy to interpret:\n",
        "\n",
        "b. Multiple Linear Regression:\n",
        "\n",
        "i). Intercept = value of Y when all independent variables are 0.\n",
        "\n",
        "ii). May be theoretical or unrealistic, especially if 0 is outside the data range.\n",
        "\n",
        "#16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "Ans. The slope in regression analysis represents the rate of change of the dependent variable (Y) with respect to the independent variable (X).\n",
        "\n",
        "a. The slope shows how much the dependent variable changes for a one-unit increase in the independent variable.\n",
        "\n",
        "b. It directly affects predictions because it determines the rate of change of the regression line.\n",
        "\n",
        "#17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "Ans- a. The intercept represents the expected value of Y when X = 0.\n",
        "\n",
        "b. It provides a baseline or starting point for understanding the relationship.\n",
        "\n",
        "#18. What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "Ans- a. It does not indicate causation.\n",
        "\n",
        "b. Always increases when more variables are added.\n",
        "\n",
        "c. Does not detect overfitting.\n",
        "\n",
        "d. Does not show whether assumptions are violated.\n",
        "\n",
        "#19. How would you interpret a large standard error for a regression coefficient?\n",
        "Ans- a. The coefficient estimate is less precise.\n",
        "\n",
        "b. Indicates high variability or insufficient data.\n",
        "\n",
        "c. Makes the coefficient less statistically reliable.\n",
        "\n",
        "#20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "Ans- a. Identified when residuals show a funnel or cone shape.\n",
        "\n",
        "b. Important because it:\n",
        "\n",
        "i) Biases standard errors.\n",
        "\n",
        "ii). Affects hypothesis testing and confidence intervals.\n",
        "\n",
        "#21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "Ans- a. Some predictors do not contribute meaningfully.\n",
        "\n",
        "b. The model may include irrelevant variables.\n",
        "\n",
        "c. Indicates possible overfitting.\n",
        "\n",
        "#22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "Ans- a. Helps when variables have different units or scales.\n",
        "\n",
        "b. Improves numerical stability.\n",
        "\n",
        "c. Necessary for regularization techniques (Ridge, Lasso).\n",
        "\n",
        "#23. What is polynomial regression?\n",
        "Ans- a. Polynomial regression models non-linear relationships.\n",
        "\n",
        "b. Uses polynomial terms of the independent variable.\n",
        "\n",
        "#24. How does polynomial regression differ from linear regression?\n",
        "Ans- a. Linear regression fits a straight line.\n",
        "\n",
        "b. Polynomial regression fits a curved line.\n",
        "\n",
        "c. Polynomial regression can capture complex patterns.\n",
        "\n",
        "#25. When is polynomial regression used?\n",
        "Ans- a. When data shows curvature.\n",
        "\n",
        "b. When linear regression underfits the data.\n",
        "\n",
        "c. When relationship is non-linear but continuous.\n",
        "\n",
        "#26. What is the general equation for polynomial regression?\n",
        "Ans-\n",
        "\n",
        "Y = b0 + b1X + b2X2 + b3X3 +‚ãØ+ bnXn\n",
        "\n",
        "#27. Can polynomial regression be applied to multiple variables?\n",
        "Ans - Yes\n",
        "\n",
        "a. Polynomial terms can be added for each independent variable.\n",
        "\n",
        "b. Interaction terms can also be included.\n",
        "\n",
        "#28. What are the limitations of polynomial regression?\n",
        "Ans- a. Risk of overfitting.\n",
        "\n",
        "b. Poor extrapolation outside data range.\n",
        "\n",
        "c. Difficult interpretation for high-degree polynomials.\n",
        "\n",
        "#29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "Ans- a. Cross-validation.\n",
        "\n",
        "b. Adjusted R¬≤.\n",
        "\n",
        "c. AIC / BIC.\n",
        "\n",
        "d. Mean Squared Error (MSE).\n",
        "\n",
        "#30. Why is visualization important in polynomial regression?\n",
        "Ans - a. Helps detect overfitting or underfitting.\n",
        "\n",
        "b. Shows how well the curve fits the data.\n",
        "\n",
        "c. Aids in choosing the correct polynomial degree.\n",
        "\n",
        "#31. How is polynomial regression implemented in Python?\n",
        "Ans- a. Use NumPy or scikit-learn.\n",
        "\n",
        "b. Steps:\n",
        "\n",
        "i). Create polynomial features.\n",
        "\n",
        "ii). Fit a linear regression model.\n",
        "\n",
        "iii). Evaluate model performance.\n",
        "\n",
        "c. Example tools:\n",
        "\n",
        "i). PolynomialFeatures from sklearn.preprocessing.\n",
        "\n",
        "ii). LinearRegression from sklearn.linear_model.\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n"
      ],
      "metadata": {
        "id": "eSW3mv_TkN0e"
      }
    }
  ]
}